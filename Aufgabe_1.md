# Aufgabe MALIS 21.3 WPM_T9.1: Data Science / Data Librarianship / IT-Praxis
## Beschreibung von datenintensiven und datenfokussierten Aktivitäten im eigenen Arbeitsalltag / in der eigenen Institution
von Sarah Eschmann

### Datenintensive und datenfokussierte Prozesse in der BLB
In der Badischen Landesbibliothek (BLB) werden an vielen Stellen datenintensive Aktivitäten durchgeführt. Da ich jedoch nur in den beiden Bereichen "Pflichtexemplare" und "Teaching Library" einen tieferen Einblick habe, werden einige Aufgaben nur aufgezählt, ohne dass tiefer darauf eingegangen wird.

#### Dateninstensive Prozesse im Bereich Pflichtexemplare
Ein Beispiel für einen datenintensiven Prozess im Pflichtexemplar-Bereich ist das Auswerten der Neuerscheinungen auf relevante Pflichtexemplare. Hierfür wird das Tool eMAS verwendet, das von der Württembergischen Landesbibliothek programmiert wurde. Das Programm wertet den Neuerscheinungsdienst der DNB der Reihen A, B und O - also Publikationen, die im Verlagsbuchhandel erscheinen, Literatur außerhalb des Verlagsbuhchandels und Online-Publikationen - mithilfe der Verlagsorte aus und filtert so nur die für Baden-Württemberg relevanten Neuerscheinungen heraus. Dies passiert ein mal wöchentlich. Zwei mal wöchentlich gleicht eMAS die von ihm erzeugte Liste mit dem Bestand der BLB in der Datenbakn des K10+ ab und entfernt die Titel aus seiner Liste, an denen die BLB bereits einen Exemplarsatz hat. 
Im Bereich der gedruckten Prflichtexemplare muss die von eMAS erzeugte Liste nun noch intellektuell mit dem Bestand im K10+ abgeglichen werden, da das Programm an manchen Stellen anfällig für ein paar Fehler ist. Beispielsweise können Nachdrucke oft nicht sicher identifiziert werden, da sich die Kataloisate für das Original und für den Nachdruck zu sehr gleichen und das Programm dadurch nicht sicher ausschließen kann, ob ein Titel schon vorhanden ist oder nicht. Zudem stellen Ortsnamen, die es nicht nur einmalig gibt ein Problem dar. Da in den Katalogisaten nur der Ortsname ohne weitere Angaben wie, zum Beispiel dem Bundesland, angegeben wird, kann eMAS nicht herausfinden, um welchen Ort es sich genau handelt. Ein Beispiel hierfür wäre Birkenfeld. Diesen Ortsnamen gibt es in Baden-württemberg, Rheinland-Pfalz und Bayern. 
Durch den intellektuellen Abgleich besteht jedoch die Möglichkeit, dem Programm "beizubringen" um ob es sich bei einer bestimmten Kombination von Verlagsort und Verlag um einen Verlag aus Baden-Württemberg handelt. Dadurch verringert sich die Fehlermenge im Laufe der Zeit immer mehr.

#### Datenintensive Prozesse im Bereich Teaching Library
In der Teaching Library gibt es einige Prozesse, die datenintensiv sind. Beispielsweise das Führen des Schulungsplans. Der Schulungsplan ist eine Excel-Tabelle, in der alle wichtigen Informationen zu geplanten und auch bereits durchgeführten Schulungen eines Jahres vermerkt sind. Der Plan enthält zum Beispiel Informationen über das Schulungsdatum, die Gruppengröße, Kontaktdaten zur Schule und zur begleitenden Lehrkraft, aber auch welche Art von Schulung gewünscht ist und ob es eventuelle Sonderwünsche gibt. Auch Organisatorisches wie zum Beispiel eine noch ausstehende Bestätigung des Schulungstermins durch die Lehrkraft oder bereitgestellte Schulungsmaterialien werden vermerkt. Da dieser Plan immer für ein ganzes Jahr geführt wird, entsteht hier eine sehr große Datenmenge.
Über diese Schulungen wird quartalsweise auch eine Statistik geführt und ausgewertet. Dies geschieht mithilfe von Excel-Tabellen und Google, da die Feedback-Formulare der BLB, die in die Auswertung mit einbezogen werden, über Google Docs erstellt wurden.
Ein Teil dieser Auswertung betrifft die Nutzungsstatistik des Online-Schulungs-Moduls, welches ich als Beispielprozess in einem eigenen Kapitel ausführlicher vorstellen werde.

#### Weitere datenintensive Prozesse in der BLB
Ein weiterer Prozess, der große Datenmengen beinhaltet, ist das Erstellen und Pflegen von Katalogisaten. Da jedes neue Katalogisat neue Daten in die K1ß0+-Datenbank bringt, wächst diese Datenmenge ständig an. 
Dies betrifft auch das Metadatenmanagement, bei dem oft große Mengen an Katalogisaten angepasst oder ergänzende Kategorien eingespielt werden. Vor Kurzem wurden beispielsweise im Projekt [bwLastCopies](https://wiki.bsz-bw.de/display/LASTCOPIES/Kurzbeschreibung+Projekt+bwLast+Copies) Listen mit allen badischen Ortsnamen in möglichst allen möglichen Schreibweisen erstellt und diese mit der K10+-Datenbank abgeglichen. So konnten nachträglich eine sehr große Anzahl an badischen Pflichtexemplaren identifiziert werden. Diese badischen Pflichtexemplare konnten dann in einem gemeinsamen Prozess durch eine neue Kategorie gekennzeichnet werden.

Ein anderer Bereich, in dem datenintensive Prozesse aufkommen, ist der Bereich der Netzpublikationen, da hier E-Books zum Beispiel in großen Datenpaketen gekauft werden. Der Zugang der E-Books erfolgt jedoch nicht aussließlich über den Kauf, denn auch hier greift das Pflichtexemplargesetzt. Dies bedeutet, dass alle badischen Verlage ihre E-Books an die BLB abliefern müssen. Dies läuft über eine Schnittstelle, über die die Verlage ihre Daten hochladen können.

### Beispielprozess mit Verbesserungspotenzial
Als Beispielprozess, in dem ich Verbesserungspotenzial sehe, habe ich die Auswertung des [Online-Lernmoduls](https://elearning.blb-karlsruhe.de/) der BLB in Moodle ausgewählt.
Diese Auswertung wird von mir einmal im Monat durchgeführt, es ist also ein regelmäßiger Prozess. Aktuell ist das Vorgehen so, dass ich in Moodle den Kurs auswähle, dessen Nutzung ich auswerten möchte und mir dann eine Datei über die gesamte Nutzung aller Gast-User seit Erstellung des Kurses ausgeben lasse. Bei den Dateiformaten gibt es die Auswahl zwischen einer csv-Datei, Excel, einer HTML-Tabelle, einer json-Datei, einer ods-Datei oder einem PDF. Ich nutze bisher immer die Excel-Datei. In dieser Excel-Datei sind dann alle Schritte, die jeder Gast-User jemals in diesem Kurs gemacht hat als einzelne Zeilen aufgeführt. Je nach Nutzung können das mehrere zehntausend Zeilen pro Monat sein. Der erste Schritt ist nun, die Zeilen zu löschen, die den auszuwertenden Monat nicht betreffen. In Moodle gäbe es zwar theoretisch die Möglichkeit, einen bestimmten Monat für die Nutzungsstatistik auszuwählen, dies funktioniert in der Praxis jedoch nicht. 
Sind nun nur noch die Zeilen für den aktuellen Monat übrig, lösche ich die Spalten, die zur Auswertung nicht benötigt werden. Zum Beispiel sind dies die Uhrzeit der Nutzung oder welcher Schritt im Lernmodul ausgeführt wurde. Der letzte Schritt ist nun, die Dubletten in der Spalte mit den anonymisierten IP-Adressen der Nutzer zu löschen, sodass ich sehe, von wie vielen IP-Adressen auf das Modul zugegriffen wurde. Daraus schließe ich dann die annäherungsweisen Nutzungszahlen. Dies muss für jeden Kurs, für den man Nutzungszahlen möchte, einzeln durchgeführt werden.
Dieser Prozess ist trotz der Funktionen in Excel ziemlich zeitaufwändig und bietet daher ein großes Verbesserungspotenzial. 
In Zukunft wäre es eine Möglichkeit, statt der Excel-Datei die json-Datei zu nutzen und den Vorgang mithilfe der gelernten Inhalte dess Wahlpflichtmoduls zu vereinfachen. Man könnte sich zum Beispiel mithilfe der passenden Befehle in Python direkt nur die Spalten der Tabelle anzeigen lassen, die man für die Auswertung auch tatsächlich braucht. Dies wären die Spalten "Datum" und "IP-Adresse". Der Befehl hierzu könnte lauten: apcs_datum_und_IP = apcs [["Datum", "IP-Adresse"]] 
Dies würde schon etwas Zeit einsparen. 
Eine weitere Hilfe wäre es, wenn man die Zeilen löschen könnte, die nicht den gewünschten Monat betreffen, indem man durch einen Befehl alle zeilen löschen könnte, die in der Spalte "Datum" einen bestimmten Wert nicht besitzen. Zum Beispiel ein Datum zwischen dem 01.05.2022 und dem 31.05.2022 für die Auswertung des Monats Mai. Eventuell könnte man dies über bedingte Anweisungen lösen.
Auch wäre es wünschenswert, die Dubletten in der Spalte "IP-Adressen" in einem automatisierten Prozess löschen zu können.  
Eine weitere Hilfe, diesen Prozess zu optimieren, wäre es, wenn man die einzelnen Tabellen für die verschiedenen Kurse zusammenfügen und in einem Schritt auswerten könnte, sodass man dann nach den einzelnen Kursen filtern kann. 
Diese letzen Verbesserungsvorschläge übersteigen jedoch noch mein aktuelles Wissen im IT-Bereich. Aber ich hoffe, diesen Beispielprozess bald verbessern und und zeitsparender durchführen zu können.
